---
title: "2_Prediction"
author: "Shiya"
date: "Nov 18, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("C:/Users/YIQ/Desktop/PIAAC/2_Prediction")
load("C:/Users/YIQ/Desktop/PIAAC/1_EDA/.RData")
```

#------------------------------------------------------------------------------------------------------------------------------------------------------
## RQ2-b 
### Build Linear Regression models to investigate the extent skills used at work are associated with full-time professionals' PS-TRE proficiency scores.
PS-TRE Model 1.0: Socio-Demographic Background (Overall)
```{r}
dat.all$GENDER<-as.factor(dat.all$GENDER)
dat.all$AGE10BAND<-as.factor(dat.all$AGE10BAND)
dat.all$PARED<-as.factor(dat.all$PARED)
dat.all$EDU<-as.factor(dat.all$EDU)

PSL.lm1.0<-lm(dat.all$mean.PSL10~GENDER+AGE10BAND+PARED+EDU,data=dat.all)
PSL.lm1.0
summary(PSL.lm1.0)

if(!("sjPlot" %in% installed.packages()[,1])) {
 install.packages("sjPlot")
 }
library(sjPlot)

sjp.lm(PSL.lm1.0,type="coef")
```

PS-TRE Model 1: Socio-Demographic Background (By Country)
```{r}
if(!("lme4" %in% installed.packages()[,1])) {
 install.packages("lme4")
 }
library(lme4)

PSL.lm1<-lmList(mean.PSL10~GENDER+AGE10BAND+PARED+EDU| CNTRYID, data=dat.all)
PSL.lm1
sapply(PSL.lm1,function(x) summary(x)$r.squared)
```


PS-TRE Model 2.0: Socio-Demographic Background + Occupational Categories (Overall)
```{r}
dat.all$Sector<-as.factor(dat.all$Sector)
dat.all$Role<-as.factor(dat.all$Role)
dat.all$ISCO2C<-as.factor(dat.all$ISCO2C)

PSL.lm2.0<-lm(dat.all$mean.PSL10~GENDER+AGE10BAND+PARED+EDU +ISCO2C,data=dat.all)
PSL.lm2.0
summary(PSL.lm2.0)

library(sjPlot)
sjp.lm(PSL.lm2.0,type="coef")
```

PS-TRE Model 2: Socio-Demographic Background + Job Characteristics (By Country)
```{r}
library(lme4)
PSL.lm2<-lmList(mean.PSL10 ~ GENDER+AGE10BAND+PARED+EDU +ISCO2C | CNTRYID, data=dat.all)
PSL.lm2
sapply(PSL.lm2,function(x) summary(x)$r.squared)
```


PS-TRE Model 3.0: Socio-Demographic Background + Occupational Categories + Info-Processing Skills USE (Overall)
```{r}
PSL.lm3.0<-lm(dat.all$mean.PSL10~GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK,data=dat.all)
PSL.lm3.0
summary(PSL.lm3.0)

library(sjPlot)
sjp.lm(PSL.lm3.0,type="coef")
```

PS-TRE Model 3: Socio-Demographic Background + Job Characteristics + Info-Processing Skills USE (By Country)
```{r}
library(lme4)
PSL.lm3<-lmList(mean.PSL10 ~ GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK | CNTRYID, data=dat.all)
PSL.lm3
sapply(PSL.lm3,function(x) summary(x)$r.squared)
```


PS-TRE Model 4.0: Socio-Demographic Background + Job Characteristics + Info-Processing Skills USE + Workplace Skills USE (Overall)
```{r}
PSL.lm4.0<-lm(dat.all$mean.PSL10~GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK + INFLUENCE+PLANNING+TASKDISC+LEARNATWORK,data=dat.all)
PSL.lm4.0
summary(PSL.lm4.0)

library(sjPlot)
sjp.lm(PSL.lm4.0,type="coef")
```

PS-TRE Model 4: Socio-Demographic Background + Job Characteristics + Info-Processing Skills USE + Workplace Skills USE (By Country)
```{r}
library(lme4)
PSL.lm4<-lmList(mean.PSL10 ~ GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK | CNTRYID, data=dat.all)
PSL.lm4

sapply(PSL.lm4,function(x) summary(x)$r.squared)
```


PS-TRE Model 5.0: Socio-Demographic Background + Job Characteristics + Info-Processing Skills USE + Workplace Skills USE + AET Participation (Overall)
```{r}
dat.all$AET_Par<-as.factor(dat.all$AET_Par)

PSL.lm5.0<-lm(dat.all$mean.PSL10~GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,data=dat.all)
PSL.lm5.0
summary(PSL.lm5.0)

library(sjPlot)
sjp.lm(PSL.lm5.0,type="coef")
```

PS-TRE Model 5: Socio-Demographic Background + Job Characteristics + Info-Processing Skills USE + Workplace Skills USE + AET Participation (By Country)
```{r}
library(lme4)
PSL.lm5<-lmList(mean.PSL10 ~ GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par| CNTRYID, data=dat.all)
PSL.lm5
sapply(PSL.lm5,function(x) summary(x)$r.squared)
```

#------------------------------------------------------------------------------------------------------------------------------------------------------
## RQ2-b.
### Build linear regression models to investigate the extent skills used at work are associated with full-time professionals' PS-TRE proficiency scores.
The coefficients of logistic regression models are called log-odds ratios.
For easy interpretation, we generate odds ratios by exponentiating log-odds ratios.

PS-TRE Model 0: Country (Overall)
Here we see that the estimated odds of socring at the top (25%) increases 63% (O.R.=1.63, 95% CI: ????) if the respodent's country is SWE and this is statistically significant (p<.001).
```{r}
dat.all$CNTRYID<-as.factor(dat.all$CNTRYID)

quantile(dat.all$mean.PSL10,.75,na.rm=TRUE)

if(!("Hmisc" %in% installed.packages()[,1])) {
 install.packages("Hmisc")
 }
library(Hmisc)

dat.all$PSL10.top.quar<-cut2(dat.all$mean.PSL10,324.1575)

PSL.glm0<-glm(dat.all$PSL10.top.quar~CNTRYID,data=dat.all,family="binomial")
PSL.glm0
summary(PSL.glm0)

library(sjPlot)
sjp.glm(PSL.glm0)
```

PS-TRE Model 1: Country + Socio-Demographic Background (Overall)
```{r}
dat.all$GENDER<-as.factor(dat.all$GENDER)
dat.all$AGE10BAND<-as.factor(dat.all$AGE10BAND)
dat.all$PARED<-as.factor(dat.all$PARED)
dat.all$EDU<-as.factor(dat.all$EDU)

PSL.glm1<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU,data=dat.all,family="binomial")
PSL.glm1
summary(PSL.glm1)

library(sjPlot)
sjp.glm(PSL.glm1)
```

PS-TRE Model 2: Country + Socio-Demographic Background + Occupational Categories (Overall)
```{r}
dat.all$Sector<-as.factor(dat.all$Sector)
dat.all$Role<-as.factor(dat.all$Role)
dat.all$ISCO2C<-as.factor(dat.all$ISCO2C)

PSL.glm2<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU + ISCO2C,data=dat.all,family="binomial")
PSL.glm2
summary(PSL.glm2)

library(sjPlot)
sjp.glm(PSL.glm2)
```

PS-TRE Model 3: Country + Socio-Demographic Background + Occupational Categories + Info-Processing Skills USE (Overall)
```{r}
PSL.glm3<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK,data=dat.all,family="binomial")
PSL.glm3
summary(PSL.glm3)

library(sjPlot)
sjp.glm(PSL.glm3)
```

PS-TRE Model 4: Country + Socio-Demographic Background + Occupational Categories + Info-Processing Skills USE + Workplace Skills USE
```{r}
PSL.glm4<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK + INFLUENCE+PLANNING+TASKDISC+LEARNATWORK,data=dat.all,family="binomial")
PSL.glm4
summary(PSL.glm4)

library(sjPlot)
sjp.glm(PSL.glm4)
```

PS-TRE Model 5: Country + Socio-Demographic Background + Occupational Categories + Info-Processing Skills USE + Workplace Skills USE + AET Participation

```{r}
PSL.glm5<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK + INFLUENCE+PLANNING+TASKDISC+LEARNATWORK + AET_Par,data=dat.all,family="binomial")
PSL.glm5
summary(PSL.glm5)

library(sjPlot)
sjp.lm(PSL.glm5)
```

### Model Selection: Stepwise Forward Selection
1. Fit a baseline model with only CNTRYID in it.
2. Add one variable at a time.
3. Compare (nested) models with and without the variable.

```{r}
PSL.glm0<-glm(dat.all$PSL10.top.quar~CNTRYID,data=dat.all,family="binomial")

add1(PSL.glm0,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

Here we can see, several variables have the same p-value. For full transparency and reproducibility, we simply/arbitarily follow the order of entry to break ties.

```{r}
fit1.glm<-update(PSL.glm0, .~. + GENDER)

add1(fit1.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit2.glm<-update(fit1.glm, .~. + AGE10BAND)

add1(fit2.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit3.glm<-update(fit2.glm, .~. + EDU)

add1(fit3.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit4.glm<-update(fit3.glm, .~. + ISCO2C)

add1(fit4.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit5.glm<-update(fit4.glm, .~. + PARED)

add1(fit5.glm,scope= ~ . + GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit6.glm<-update(fit5.glm, .~. + NUMWORK)

add1(fit6.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit7.glm<-update(fit6.glm, .~. + AET_Par)

add1(fit7.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit8.glm<-update(fit7.glm, .~. + READWORK)

add1(fit8.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit9.glm<-update(fit8.glm, .~. + ICTWORK)

add1(fit9.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit10.glm<-update(fit9.glm, .~. + WRITWORK)

add1(fit10.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

```{r}
fit11.glm<-update(fit10.glm, .~. + LEARNATWORK)

add1(fit11.glm,scope= ~ . +GENDER+AGE10BAND+PARED+EDU + ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK +INFLUENCE+PLANNING+TASKDISC+LEARNATWORK +AET_Par,test="Chisq")
```

The "Best" Model from Forward Selection:
```{r}
PSL.glm.best<-glm(dat.all$PSL10.top.quar~CNTRYID + GENDER + AGE10BAND + EDU + 
    ISCO2C + PARED + NUMWORK + AET_Par + READWORK + ICTWORK + WRITWORK + LEARNATWORK,data=dat.all,family="binomial")
summary(PSL.glm.best)

library(sjPlot)
sjp.lm(PSL.glm.best)
```

### Model Selection: Backward Selection
#### Manual F-test-based backward selection
```{r}
PSL.glm5<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK + INFLUENCE+PLANNING+TASKDISC+LEARNATWORK + AET_Par,data=dat.all,family="binomial")
drop1(PSL.glm5,test="F")
```

```{r}
drop1(update(PSL.glm5, ~ . -INFLUENCE),test="F")
```

```{r}
drop1(update(PSL.glm5, ~ . -INFLUENCE -PLANNING),test="F")
```

```{r}
drop1(update(PSL.glm5, ~ . -INFLUENCE -PLANNING -TASKDISC),test="F")
```

Now all variables are significant at p<.05.
The final model is the same as the "best" model from forward section:
```{r}
summary(update(PSL.glm5, ~ . -INFLUENCE -PLANNING -TASKDISC))
```

#### Manual likelihood-ratio-test-based backward selection
```{r}
PSL.glm.null<-glm(dat.all$PSL10.top.quar~1,data=dat.all,family="binomial")

PSL.glm5<-glm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK + INFLUENCE+PLANNING+TASKDISC+LEARNATWORK + AET_Par,data=dat.all,family="binomial")

drop1(PSL.glm5,test="LRT")
```

```{r}
drop1(update(PSL.glm5, ~ . -INFLUENCE),test="LRT")
```

```{r}
drop1(update(PSL.glm5, ~ . -INFLUENCE -PLANNING),test="LRT")
```

```{r}
drop1(update(PSL.glm5, ~ . -INFLUENCE -PLANNING -TASKDISC),test="LRT")
```

Now all variables are significant at p<.05.
The final model (& AIC) is the same as the "best" model from forward section:
```{r}
summary(update(PSL.glm5, ~ . -INFLUENCE -PLANNING -TASKDISC))
```

There are many other ways to do model selection, including automated procedures. Although we would not use these automated methods as a first line of model selection, they can be useful for seeing if the final result is robust to alternative procedures.

#### Automated likelihood-ratio-test-based backward selection
```{r}
if(!("rms" %in% installed.packages()[,1])) {
 install.packages("rms")
 }
library(rms)

lrm.PSL.glm5<-rms::lrm(dat.all$PSL10.top.quar~CNTRYID +GENDER+AGE10BAND+PARED+EDU +ISCO2C +READWORK+WRITWORK+NUMWORK+ICTWORK + INFLUENCE+PLANNING+TASKDISC+LEARNATWORK + AET_Par,data=dat.all)

fastbw(lrm.PSL.glm5,rule="p",sls=.05)
```

## Summary of Logistic Regression:
Remember the "best" model selected by both forward and backward selction:
```{r}
if(!("Hmisc" %in% installed.packages()[,1])) {
 install.packages("Hmisc")
 }
library(Hmisc)

if(!("sjPlot" %in% installed.packages()[,1])) {
 install.packages("sjPlot")
 }
library(sjPlot)

if(!("plyr" %in% installed.packages()[,1])) {
 install.packages("plyr")
 }
library(plyr)

dat.all$PSL10.top.quar<-cut2(dat.all$mean.PSL10,324.1575)

PSL.glm.best<-glm(dat.all$PSL10.top.quar~CNTRYID + GENDER + AGE10BAND + EDU + 
    ISCO2C + PARED + NUMWORK + AET_Par + READWORK + ICTWORK + WRITWORK + LEARNATWORK,data=dat.all,family="binomial")

summary(PSL.glm.best)

sjp.lm(PSL.glm.best)
```

Let's add a new column for predicted values and then plot the ditribution of these predictions:
```{r}
dat.all$PSL10.top.quar.Pred<-predict(PSL.glm.best,newdata=dat.all,type="response")
hist(dat.all$PSL10.top.quar.Pred,breaks=11)
```

Here we see that the selected model predicted that most full-time professionals have a farily low probability of scoring above the 75th percentile (325.1575) in PS-TRE.
To evaluate the accuaracy (how often the prediction matched the actual outcome), we need to specify a cutoff above which we make a binary prediction that the respondent is likely to score above the 75th percentile, and below which we predict that the respondent will score below the 75th percentile.
Let's say, we set the cut-off at .5 and then see how the outcomes are distributed across our two predictions:
```{r}
aaa<-dat.all$PSL10.top.quar
new.levels<-c("No","Yes")
levels(aaa)<-new.levels
dat.all$PSL10.top.quar<-aaa

dat.all$PSL10.top.quar.Pred.5<-dat.all$PSL10.top.quar.Pred>.5

PredTab<-table(dat.all$PSL10.top.quar.Pred.5,dat.all$PSL10.top.quar=="Yes",dnn=c("Prediction","PS-TRE Score above 75th percentile"))
PredTab
sum(diag(PredTab))/sum(PredTab)
```
The accuracy is .753. 
Note that we are making predictions on the same observations we used to train our model. This can make the performance a little too optimisitic. 
In other words, we are optimizing the performance of this training dataset, and the performance may not be as good when it is applied to unseen data.

## Training & Testing
To explore this a little further. We randomly divide the dataset into two subsets: dat.Train and dat.Test so that 50% of the data is in the training dataset and 50% is in the testing dataset.
```{r}
set.seed(123)

if(!("caret" %in% installed.packages()[,1])) {
 install.packages("caret")
 }
library(caret)
```

```{r}
trainIdx<-createDataPartition(dat.all$PSL10.top.quar,p=.5)$Resample1
dat.Train<-dat.all[trainIdx,]
dat.Test<-dat.all[-trainIdx,]
```

First, only the TRAINING data is used to fit the model:
```{r}
PSL.glm.train<-glm(PSL10.top.quar~CNTRYID + GENDER + AGE10BAND + EDU + 
    ISCO2C + PARED + NUMWORK + AET_Par + READWORK + ICTWORK + WRITWORK + LEARNATWORK,data=dat.Train,family="binomial")
summary(PSL.glm.train)
```

Let's create predictions for this model in the TRAINING dataset:
```{r}
dat.Train$PSL10.top.quar.Pred<-predict(PSL.glm.train,newdata=dat.Train,type="response")
dat.Train$PSL10.top.quar.Pred.5<-dat.Train$PSL10.top.quar.Pred>.5

PredTab.Train<-table(dat.Train$PSL10.top.quar.Pred.5,dat.Train$PSL10.top.quar=="Yes",dnn=c("Prediction","PS-TRE Score above 75th percentile"))
PredTab.Train
sum(diag(PredTab.Train))/sum(PredTab.Train)
```
Accuracy is very similar to that when using the whole dataset: .760 (vs .753).

Now, let's try the model/prediction algorithm on the TESTING data.
This dataset was not used to build the model on, so it is a better assessment of the performance of the prediction algorithm.
```{r}
dat.Test$PSL10.top.quar.Pred<-predict(PSL.glm.train,newdata=dat.Test,type="response")
dat.Test$PSL10.top.quar.Pred.5<-dat.Test$PSL10.top.quar.Pred>.5

PredTab.Test<-table(dat.Test$PSL10.top.quar.Pred.5,dat.Test$PSL10.top.quar=="Yes",dnn=c("Prediction","PS-TRE Score above 75th percentile"))
PredTab.Test
sum(diag(PredTab.Test))/sum(PredTab.Test)
```
Here the accuracy is .746 which is slightly less than that in the training dataset.  
The discrepancy between training and test dataset performance is very small.


## ROC curves
The use of .5 as a threshold is completely arbitrary, and may not be a good cutoff for our particular application.

Looking back at the Testing dataset, for those who score below 75th percentile, our prediction is correct in 2199 cases or 2199/2423=91% of cases. As for those who score above 75th percentile, our prediction is only correct in 277/895=31% of cases. 
Our accuracy measure is only good, because we have far more respondents who score below 75th percentile than above.
These two quantities we calculated are often called Specificity and Sensitivity.

To get around the arbitariness of the .5 cutoff, we introduce the ROC curve to evaluate all potential cutoffs.
ROC curves plot 1-Specificity vs. Sensitivity of the algorithm for predicting the outcome, while varying the cutoffs used to define the predcitions. 
Evaluation is usually done with AUC -- 1 indication perfect prediction and .5 being no better than chance. 

For our TRAINING and TESTING datasets, the ROC curves and AUCs are presented below.
```{r}
if(!("ROCR" %in% installed.packages()[,1])) {
 install.packages("ROCR")
 }
library(ROCR)

complete.dat.Train<-dat.Train[complete.cases(dat.Train[c("PSL10.top.quar.Pred","PSL10.top.quar")]),]

PredTrain<-prediction(complete.dat.Train$PSL10.top.quar.Pred,complete.dat.Train$PSL10.top.quar)
PerfTrain<-performance(PredTrain,"tpr","fpr")
plot(PerfTrain)
text(.6, .2, paste0("AUC: ",round(performance(PredTrain,"auc")@y.values[[1]],3)))

complete.dat.Test<-dat.Test[complete.cases(dat.Test[c("PSL10.top.quar.Pred","PSL10.top.quar")]),]

PredTest<-prediction(complete.dat.Test$PSL10.top.quar.Pred,complete.dat.Test$PSL10.top.quar)
PerfTest<-performance(PredTest,"tpr","fpr")
lines(PerfTest@x.values[[1]],PerfTest@y.values[[1]],col='red')
text(.6, .1, paste0("AUC: ",round(performance(PredTest,"auc")@y.values[[1]],3)),col='red')
```
Here we can see a few important things:
1. As expected, the AUC for the testing dataset (red) is slightly lowere than thhe AUC for the training dataset (black).
2. There is a trade off between Sensitivity (true positive) and Specificity (1- false positive). For example, if we need 80% sensitivity, we would have a false positive rate of about 40% or about 60% specificity. If we need 90% sensitivity, we would have a false positive rate of about 60% or about 40% specificity.


## Calibration
ROC curves tell us about discrimination (how well we are able to distingushi between those who score belwo and above 75th percentile), but an equally important aspect is the calibration of our model.
```{r}
if(!("gbm" %in% installed.packages()[,1])) {
 install.packages("gbm")
 }
library(gbm)

prop.table(table(complete.dat.Train$PSL10.top.quar,cut2(complete.dat.Train$PSL10.top.quar.Pred,seq(0,1,0.1))),2)

gbm::calibrate.plot(complete.dat.Train$PSL10.top.quar,complete.dat.Train$PSL10.top.quar.Pred)
```
```{r}
prop.table(table(complete.dat.Test$PSL10.top.quar,cut2(complete.dat.Test$PSL10.top.quar.Pred,seq(0,1,0.1))),2)

gbm::calibrate.plot(complete.dat.Test$PSL10.top.quar,complete.dat.Test$PSL10.top.quar.Pred)
```

More formal testing can be done using the Hosmer-Lemeshow goodness-of-fit test.
Note that the null hypothesis is that the model fits the observed data.
The reported p-value is greater than .05, indicating no significant difference between the model and the oberved data. Our model is well-fitting.
```{r}
if(!("sjstats" %in% installed.packages()[,1])) {
 install.packages("sjstats")
 }
library(sjstats)

hoslem_gof(PSL.glm.train,g=10)
```

